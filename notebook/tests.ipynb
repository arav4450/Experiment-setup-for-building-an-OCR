{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] The specified module could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from codebase import lit_model,data, model,training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"E:\\Coding Environment\\ocr\\codebase\\training\"\n",
    "curr_dir = os.getcwd()\n",
    "os.chdir(path)\n",
    "opt = training.get_config(\"config.yaml\")\n",
    "os.chdir(curr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:122: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  \"DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n",
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:141: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  \"DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\"\n"
     ]
    }
   ],
   "source": [
    "data_obj = data.datamodule(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    }
   ],
   "source": [
    "data_obj.setup()\n",
    "x, y = next(iter(data_obj.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 64, 600])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ken-Telemadrid', '2-861 Cvd', \"Warner } bar 'joke'\", 'Nikkie')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = model.Model(opt)\n",
    "#model_obj = model_obj.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model_class = lit_model.BaseLitModel\n",
    "lit_mode = lit_model_class(opt=opt, model=model_obj,args = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1585: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  \"GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\"\n",
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | model     | Model    | 3.8 M \n",
      "1 | loss_fn   | CTCLoss  | 0     \n",
      "2 | train_acc | Accuracy | 0     \n",
      "3 | val_acc   | Accuracy | 0     \n",
      "4 | test_acc  | Accuracy | 0     \n",
      "---------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.128    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 10/56 [00:29<02:17,  2.99s/it, loss=nan, v_num=1, validation/loss=nan.0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 10/56 [00:40<03:06,  4.05s/it, loss=nan, v_num=1, validation/loss=nan.0]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer( max_epochs = 2)\n",
    "\n",
    "trainer.fit(lit_mode, datamodule=data_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                    | Type                 | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model                   | Model                | 3.8 M \n",
      "1 | model.FeatureExtraction | VGG_FeatureExtractor | 1.4 M \n",
      "2 | model.AdaptiveAvgPool   | AdaptiveAvgPool2d    | 0     \n",
      "3 | model.SequenceModeling  | Sequential           | 2.4 M \n",
      "4 | model.Prediction        | Linear               | 24.9 K\n",
      "5 | loss_fn                 | CTCLoss              | 0     \n",
      "6 | train_acc               | Accuracy             | 0     \n",
      "7 | val_acc                 | Accuracy             | 0     \n",
      "8 | test_acc                | Accuracy             | 0     \n",
      "-----------------------------------------------------------------\n",
      "3.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 M     Total params\n",
      "15.128    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 32. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n",
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:433: UserWarning: The number of training samples (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 28/56 [00:06<00:06,  4.27it/s, loss=91.3, v_num=8]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:60: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 56/56 [00:09<00:00,  5.63it/s, loss=4.38, v_num=8, validation/loss=4.360]\n"
     ]
    }
   ],
   "source": [
    "log_dir = lit_mode.log_dirname()\n",
    "logger = pl.loggers.TensorBoardLogger(log_dir)\n",
    "experiment_dir = logger.log_dir\n",
    "\n",
    "goldstar_metric = \"validation/loss\"\n",
    "filename_format = \"epoch={epoch:04d}-validation.loss={validation/loss:.3f}\"\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        save_top_k=5,\n",
    "        filename=filename_format,\n",
    "        monitor=goldstar_metric,\n",
    "        mode=\"min\",\n",
    "        auto_insert_metric_name=False,\n",
    "        dirpath=experiment_dir,\n",
    "        every_n_epochs= 1, # can be passed as an argument\n",
    "    )\n",
    "\n",
    "summary_callback = pl.callbacks.ModelSummary(max_depth=2)\n",
    "\n",
    "callbacks = [summary_callback, checkpoint_callback]\n",
    "if opt.stop_early:\n",
    "    early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "            monitor=\"validation/loss\", mode=\"min\", patience=opt.stop_early\n",
    "        )\n",
    "    callbacks.append(early_stopping_callback)\n",
    "\n",
    "trainer = pl.Trainer( callbacks=callbacks, logger=logger, accelerator = opt.accelerator, max_epochs = 25, devices = opt.devices)\n",
    "\n",
    "trainer.fit(lit_mode, datamodule=data_obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
